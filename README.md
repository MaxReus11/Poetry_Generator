# Poetry_Generator
Comparison of neural network architectures for text generation

This repository contains an experimental poetry generator built using various basic neural network technologies. The project implements and tests different approaches to text generation:

RNN (Recurrent Neural Networks)

LSTM

GRU

Use of pre-trained embeddings (Word2Vec)

Transformers and modern architectures

The goal of the project is to study the fundamentals of sequence models and compare the quality of text generation using different architectures.

Since this is one of my first projects, the focus is not on performance but on understanding the key ideas and principles behind building neural network models for text generation.

Features

Generation of poetic texts based on a trained corpus

Comparison of different architectures

Experiments with training on small and medium datasets

Simple visualization of results

Project Status

The project is currently in an active stage of training and experimentation. Improvements in text quality, addition of new architectures, and optimizations are possible.